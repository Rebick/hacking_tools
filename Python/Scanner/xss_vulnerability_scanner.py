#!/usr/bin/env python3

import argparse
import requests
import re
from urllib.parse import urljoin

class Scanner:
    def __init__(self, url, ignore_links, payloads, hide_words, hide_codes, hide_lines, max_depth):
        self.session = requests.Session()
        self.target_url = url
        self.target_links = []
        self.links_to_ignore = ignore_links
        self.payloads = payloads
        self.hide_words = hide_words
        self.hide_codes = hide_codes
        self.hide_lines = hide_lines
        self.max_depth = max_depth

    def extract_links_from(self, url):
        try:
            response = self.session.get(url)
            return re.findall('(?:href="(.*?)"|src="(.*?)")', response.text)
        except requests.RequestException as e:
            print(f"[!] Error al extraer enlaces de {url}: {e}")
            return []

    def crawl(self, url=None, directory_wordlist=None, depth=0):
        if url is None:
            url = self.target_url
        if depth > self.max_depth:
            print(f"[...] Profundidad máxima alcanzada en {url}.")
            return

        href_links = self.extract_links_from(url)

        for link in href_links:
            link = urljoin(url, link[0] or link[1])  # Usa el grupo capturado

            if "#" in link:
                link = link.split("#")[0]

            if (
                self.target_url in link
                and link not in self.target_links
                and link not in self.links_to_ignore
            ):
                try:
                    response = self.session.get(link)
                    word_count = len(response.text.split())
                    line_count = len(response.text.splitlines())

                    # Condiciones para ocultar resultados
                    if (
                        response.status_code in self.hide_codes or
                        word_count in self.hide_words or
                        line_count in self.hide_lines
                    ):
                        continue  # Si cumple alguna condición de exclusión, saltar este enlace

                    # Mostrar información del enlace descubierto
                    print(f"[+] Descubierto enlace: {link} | Status: {response.status_code} | Words: {word_count} | Lines: {line_count}")
                    self.target_links.append(link)
                    self.crawl(link, directory_wordlist, depth + 1)
                except requests.RequestException as e:
                    print(f"[!] Error al acceder a {link}: {e}")

        if directory_wordlist:
            for directory in directory_wordlist:
                potential_link = urljoin(url, directory)
                if potential_link not in self.target_links and potential_link not in self.links_to_ignore:
                    try:
                        response = self.session.get(potential_link)
                        word_count = len(response.text.split())
                        line_count = len(response.text.splitlines())

                        # Condiciones para ocultar resultados
                        if (
                            response.status_code in self.hide_codes or
                            word_count in self.hide_words or
                            line_count in self.hide_lines
                        ):
                            continue  # Si cumple alguna condición de exclusión, saltar este directorio

                        # Mostrar información del directorio descubierto
                        print(f"[+] Descubierto directorio: {potential_link} | Status: {response.status_code} | Words: {word_count} | Lines: {line_count}")

                        if response.status_code == 200:
                            self.target_links.append(potential_link)
                            self.crawl(potential_link, directory_wordlist, depth + 1)
                    except requests.RequestException as e:
                        print(f"[!] Error al intentar acceder a {potential_link}: {e}")

    def run_scanner(self):
        for link in self.target_links:
            print(f"[+] Analizando enlace: {link}")


def parse_arguments():
    parser = argparse.ArgumentParser(description="Esc\u00e1ner web con par\u00e1metros personalizados")
    parser.add_argument("target_file", help="Archivo con los objetivos")
    parser.add_argument("xss_wordlist", help="Archivo con payloads de XSS")
    parser.add_argument("directory_wordlist", help="Archivo con directorios a probar")
    parser.add_argument("--hw", type=str, help="Ocultar por n\u00famero de palabras, separadas por comas (ejemplo: 0,40)", default="")
    parser.add_argument("--hc", type=str, help="Ocultar por c\u00f3digos de estado HTTP, separadas por comas (ejemplo: 404,405)", default="")
    parser.add_argument("--hl", type=str, help="Ocultar por n\u00famero de l\u00edneas, separadas por comas (ejemplo: 20,30)", default="")
    return parser.parse_args()


def load_wordlist(wordlist_path):
    try:
        with open(wordlist_path, "r") as file:
            return [line.strip() for line in file.readlines() if line.strip()]
    except FileNotFoundError:
        print(f"[!] No se encontr\u00f3 el archivo: {wordlist_path}")
        return []


def load_targets(target_file):
    try:
        with open(target_file, "r") as file:
            targets = [line.strip() for line in file.readlines() if line.strip()]
            if len(targets) == 0:
                raise ValueError("El archivo de objetivos est\u00e1 vac\u00edo.")
            return targets
    except FileNotFoundError:
        print(f"[!] No se encontr\u00f3 el archivo de objetivos: {target_file}")
        exit()
    except ValueError as ve:
        print(f"[!] Error: {ve}")
        exit()


if __name__ == "__main__":
    args = parse_arguments()

    # Parsear valores de los par\u00e1metros
    hide_words = [int(w) for w in args.hw.split(",") if w.isdigit()]
    hide_codes = [int(c) for c in args.hc.split(",") if c.isdigit()]
    hide_lines = [int(l) for l in args.hl.split(",") if l.isdigit()]

    # Cargar datos desde archivos
    targets = load_targets(args.target_file)
    xss_payloads = load_wordlist(args.xss_wordlist)
    directory_wordlist = load_wordlist(args.directory_wordlist)

    # Configuración interactiva
    max_depth = int(input("Ingrese la profundidad máxima de recursión para el crawling (ejemplo: 2): "))
    exclude_patterns = input("Ingrese patrones a excluir durante el crawling separados por comas (ejemplo: 'images,css'): ").split(",")

    for target_url in targets:
        print(f"\n[+] Iniciando escaneo para: {target_url}")
        links_to_ignore = [urljoin(target_url, "logout.php")] + [urljoin(target_url, pattern) for pattern in exclude_patterns]

        # Inicializa el esc\u00e1ner
        vuln_scanner = Scanner(
            target_url, links_to_ignore, xss_payloads, hide_words, hide_codes, hide_lines, max_depth
        )

        # Ejecutar rastreo y an\u00e1lisis
        vuln_scanner.crawl(directory_wordlist=directory_wordlist)
        vuln_scanner.run_scanner()
