#!/usr/bin/env python3
"""
scanner_v2.py
-------------
Crawler & Quick-Scanner para CTF / Bug-Bounty.
"""

# -------------------- IMPORTS -------------------- #
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
import argparse
import requests
import re
import sys                              
from urllib.parse import urljoin
import os
try:
    import psutil           # pip install psutil
except ImportError:
    psutil = None
# -------------------- BANNER -------------------- #
def banner():
    """Muestra un banner/menú de ayuda extendido."""
    print(r"""
   ____        _                               
  / ___|  ___ | |_  ___  _ __ ___   ___  _ __  
  \___ \ / _ \| __|/ _ \| '_ ` _ \ / _ \| '_ \ 
   ___) | (_) | |_| (_) | | | | | | (_) | | | |
  |____/ \___/ \__|\___/|_| |_| |_|\___/|_| |_|
                                              
  Simple Web Crawler & Quick Scanner
  ==================================
  Este programa rastrea enlaces internos, fuerza directorios con una wordlist
  y (en próximas versiones) lanza payloads XSS/LFI/SSTI.

  USO BÁSICO:
      python3 scanner_v2.py objetivos.txt xss.txt dirs.txt [opciones]

  ARGUMENTOS POSICIONALES:
      objetivos.txt      Archivo con una URL por línea.
      xss.txt            Wordlist de payloads XSS.
      dirs.txt           Wordlist de directorios a forzar.

  OPCIONES FRECUENTES:
      --hw 0,40          Oculta respuestas con 0 o 40 palabras.
      --hc 404,403       Oculta respuestas con códigos 404 o 403.
      --hl 1,2           Oculta respuestas con 1 o 2 líneas.
      --help-menu        Muestra este menú y sale.
      -h / --help        Ayuda estándar de argparse.

  EJEMPLO:
      python3 scanner_v2.py targets.txt xss.txt common_dirs.txt \
          --hc 404 --hw 0,10
""")
def recommended_threads(io_bound=True, hard_cap=64):
    """
    Devuelve un número “sano” de hilos según CPU y RAM.
    • Para tareas I/O (web-crawling) ≈ CPU x 5.
    • Para tareas CPU (hash-cracking) ≈ CPU x 1.
    Se limita a `hard_cap`.
    """
    cores = os.cpu_count() or 2
    if io_bound:
        factor = 5          # red interna o VM local: 5-8 suele ir bien
    else:
        factor = 1
    threads = cores * factor

    if psutil:
        mem_gb = psutil.virtual_memory().total / 2**30
        # 40 MB por hilo como referencia
        mem_limit = int(mem_gb * 25)
        threads = min(threads, mem_limit)

    return min(threads, hard_cap)
# -------------------- SCANNER CLASS -------------------- #
class Scanner:
    # … (sin cambios en esta sección; recorta por brevedad)
    def __init__(self, url, ignore_links, payloads,
                 hide_words, hide_codes, hide_lines, max_depth, exclude_patterns=None, threads=None):
        self.session = requests.Session()
        self.target_url = url.rstrip("/") + "/"
        self.target_links = []
        self.links_to_ignore = ignore_links
        self.payloads = payloads
        self.hide_words = hide_words
        self.hide_codes = hide_codes
        self.hide_lines = hide_lines
        self.max_depth = max_depth
        self.exclude_patterns = exclude_patterns or []
        self.emails = set() 
        self.threads = threads or recommended_threads()
# -------------------- UTILIDADES I/O -------------------- #
# ------------------------------------------------------------------ #
    # 1) EXTRAER ENLACES (href/src) DESDE UNA URL
    # ------------------------------------------------------------------ #
    def extract_links_from(self, url):
        """
        Devuelve una lista de tuplas con las rutas encontradas en atributos
        href="…" o src="…".  Cada tupla contiene (href, src); sólo uno de los
        dos elementos vendrá poblado.
        """
        try:
            response = self.session.get(url, timeout=10)
            pattern = r'(?:href="(.*?)"|src="(.*?)")'
            return re.findall(pattern, response.text, re.IGNORECASE)
        except requests.RequestException as e:
            print(f"[!] Error al extraer enlaces de {url}: {e}")
            return []
    # ------------------------------------------------------------------ #
    # 2) CRAWL (RECUSIVO + WORDLIST DE DIRECTORIOS)
    # ------------------------------------------------------------------ #
    def crawl(self, url=None, directory_wordlist=None, depth=0):
            """
            Descubre enlaces internos de forma recursiva y, opcionalmente,
            fuerza directorios tomados de una wordlist.
            """
            # Punto de partida
            if url is None:
                url = self.target_url

            # Límite de profundidad
            if depth > self.max_depth:
                print(f"[...] Profundidad máxima alcanzada en {url}.")
                return

            # -------- 2.1  Enlaces <a>/<img>/<script> ---------------------- #
            href_links = self.extract_links_from(url)

            for link in href_links:
                link = urljoin(url, link[0] or link[1])  # Normaliza a absoluto

                if "#" in link:                          # Quita anclas
                    link = link.split("#")[0]

                if (self.target_url in link and          # Sólo internos
                    link not in self.target_links and
                    link not in self.links_to_ignore):

                    # Métricas rápidas para filtros de ocultación
                    try:
                        r = self.session.get(link, timeout=10)
                        words = len(r.text.split())
                        lines = len(r.text.splitlines())
                        self.extract_emails(r.text, link)
                    except requests.RequestException as e:
                        print(f"[!] Error al acceder a {link}: {e}")
                        continue

                    if (r.status_code in self.hide_codes or
                        words          in self.hide_words or
                        lines          in self.hide_lines):
                        continue                          # Salta por filtro

                    print(f"[+] Descubierto enlace: {link} | "
                        f"Status: {r.status_code} | Words: {words} | "
                        f"Lines: {lines}")

                    self.target_links.append(link)
                    self.crawl(link, directory_wordlist, depth + 1)

                    # -------- 2.2  Fuerza directorios de wordlist ----------------- #
            if directory_wordlist:
                # Si la URL base no es “directorio” (acaba en .css, .js, etc.), no forcejeamos
                if not url.endswith("/"):
                    return

                for directory in directory_wordlist:
                    # Construye la ruta potencial
                    potential = urljoin(url, directory.lstrip("/"))

                    # 1) Deduplicación y blacklist
                    if (potential in self.target_links or
                        potential in self.links_to_ignore or
                        any(pat.lower() in potential.lower()          # patrones a excluir
                            for pat in self.exclude_patterns)):
                        continue

                    try:
                        r = self.session.get(potential, timeout=10)
                        words = len(r.text.split())
                        lines = len(r.text.splitlines())
                        self.extract_emails(r.text, potential)  # ← nueva
                    except requests.RequestException as e:
                        print(f"[!] Error al intentar acceder a {potential}: {e}")
                        continue

                    # 2) Aplicar filtros de ocultación
                    if (r.status_code in self.hide_codes or
                        words          in self.hide_words or
                        lines          in self.hide_lines):
                        continue

                    # 3) Registrar hallazgo
                    print(f"[+] Descubierto directorio: {potential} | "
                        f"Status: {r.status_code} | Words: {words} | "
                        f"Lines: {lines}")

                    # 4) Si es accesible (200), añadir y profundizar
                    if r.status_code == 200:
                        self.target_links.append(potential)
                        self.crawl(potential, directory_wordlist, depth + 1)
        # --------------------------------------------------------------- #
    #  EXTRAER <form> DE UNA URL
    # --------------------------------------------------------------- #
    def extract_forms(self, url):
        """Devuelve una lista de objetos <form> de BeautifulSoup."""
        try:
            html = self.session.get(url, timeout=10).text
        except requests.RequestException:
            return []
        soup = BeautifulSoup(html, "html.parser")
        return soup.find_all("form")

    # --------------------------------------------------------------- #
    #  PROBAR XSS EN FORMULARIOS
    # --------------------------------------------------------------- #
    def test_xss_in_form(self, form, url):
        """
        Inyecta cada payload en campos tipo text/search y comprueba si
        aparece reflejado en la respuesta.  Devuelve True si hay XSS.
        """
        is_vuln = False
        for payload in self.payloads:
            data = {}
            for input_tag in form.find_all(["input", "textarea"]):
                name = input_tag.get("name")
                if not name:
                    continue
                typ = input_tag.get("type", "text")
                # Inyectamos payload sólo en campos aptos
                data[name] = payload if typ in ("text", "search", "hidden") else input_tag.get("value", "")
            action = form.get("action") or url
            full_url = urljoin(url, action)

            method = form.get("method", "get").lower()
            try:
                if method == "post":
                    r = self.session.post(full_url, data=data, timeout=10)
                else:
                    r = self.session.get(full_url, params=data, timeout=10)
            except requests.RequestException:
                continue

            if payload in r.text:
                print(f"[!XSS] Reflejado en formulario → {full_url}")
                is_vuln = True
                break
        return is_vuln

    # --------------------------------------------------------------- #
    #  PROBAR XSS EN PARÁMETROS GET
    # --------------------------------------------------------------- #
    def test_xss_in_link(self, url):
        """
        Si la URL contiene un parámetro (?a=), inyecta payload y busca
        reflejo.  Devuelve True si hay XSS.
        """
        if "?" not in url or "=" not in url:
            return False
        is_vuln = False
        base, sep, tail = url.partition("=")
        for payload in self.payloads:
            test_url = base + "=" + payload
            try:
                r = self.session.get(test_url, timeout=10)
            except requests.RequestException:
                continue
            if payload in r.text:
                print(f"[!XSS] Reflejado en link → {test_url}")
                is_vuln = True
                break
        return is_vuln

    def run_scanner(self):
            """
            Punto de entrada para pruebas activas (XSS, LFI, etc.).  De momento
            sólo recorre los enlaces descubiertos y los muestra.
            """
            with ThreadPoolExecutor(max_workers=self.threads) as pool:
                futures = []            
                for link in self.target_links:
                    print(f"[+] Analizando enlace: {link}")
                    self.test_xss_in_link(link)
                    for form in self.extract_forms(link):
                        self.test_xss_in_form(form, link)
                    # TODO: aquí irán test_xss_in_link(), extract_forms(), etc.
                for f in as_completed(futures):
                    pass
                if self.emails:
                    print("\n=== Correos únicos encontrados ===")
                    for e in sorted(self.emails):
                        print("  •", e)
    # --------------------------------------------------------------- #
    #  EXTRAER CORREOS DE UN CUERPO HTML                               
    # --------------------------------------------------------------- #
    def extract_emails(self, html, source_url):
        """
        Busca correos en el HTML y los agrega a self.emails.
        Muestra cada hallazgo la primera vez que aparece.
        """
        pattern = r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}"
        for email in re.findall(pattern, html):
            if email not in self.emails:
                self.emails.add(email)
                print(f"[✓] Email hallado en {source_url} → {email}")
    def _scan_forms_worker(self, url):
    for form in self.extract_forms(url):
        self.test_xss_in_form(form, url)    

# -------------------- ARGPARSE -------------------- #
def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Escáner web (crawler + filtros rápidos)",
        add_help=False                   #  <- desactiva -h auto para personalizar
        )
        # Flag estándar manual
    parser.add_argument("--threads", type=int, action="store_true",
                    help="Máximo de hilos (por defecto se calcula)")
    parser.add_argument("-h", "--help", action="store_true",
                            help="Mostrar ayuda breve y salir")
        # Menú extendido
    parser.add_argument("--help-menu", action="store_true",
                            help="Mostrar menú de ayuda extendido y salir")
        # Quiet (opcional)
    parser.add_argument("--quiet", action="store_true",
                            help="No mostrar banner al iniciar")

        # Posicionales
    parser.add_argument("target_file", nargs="?",
                            help="Archivo con los objetivos")
    parser.add_argument("xss_wordlist", nargs="?",
                            help="Archivo con payloads de XSS "
                                "(por defecto se usará la lista de SecLists)")
    parser.add_argument("--dir-wordlist", metavar="FILE", default="",
                        help="Wordlist de directorios (opcional, default "
                             "directory-list-2.3-medium.txt)")
    parser.add_argument("--page-wordlist", metavar="FILE", default="",
                        help="Wordlist de páginas/archivos para fuzzing "
                             "(opcional, default fuzz-Bo0oM.txt)")


        # Filtros de ocultación
    parser.add_argument("--hw", default="", help="Ocultar por nº palabras")
    parser.add_argument("--hc", default="", help="Ocultar por códigos HTTP")
    parser.add_argument("--hl", default="", help="Ocultar por nº líneas")
    return parser.parse_args()

def load_wordlist(wordlist_path):
        """Devuelve una lista con cada línea no vacía del archivo indicado."""
        try:
            with open(wordlist_path, "r", encoding="utf-8") as f:
                return [l.strip() for l in f 
                    if l.strip() and not l.lstrip().startswith("#")]
        except FileNotFoundError:
            print(f"[!] No se encontró: {wordlist_path}")
            return []

def load_targets(target_file):
    """Devuelve las URLs del archivo de objetivos; aborta si está vacío."""
    try:
        with open(target_file, "r", encoding="utf-8") as f:
            targets = [l.strip() for l in f if l.strip()]
            if not targets:
                raise ValueError("El archivo de objetivos está vacío.")
            return targets
    except FileNotFoundError:
        print(f"[!] No se encontró el archivo de objetivos: {target_file}")
        sys.exit(1)
    except ValueError as ve:
        print(f"[!] {ve}")
        sys.exit(1)
    
        # ------------------------------------------------------------------ #
        # 3) RUN_SCANNER (POR AHORA SÓLO IMPRIME)
        # ------------------------------------------------------------------ #
        
# -------------------- MAIN -------------------- #
if __name__ == "__main__":
    try:
        args = parse_arguments()

        # Mostrar ayudas y salir
        if args.help or args.help_menu:
            # ayuda corta → delega en argparse “print_help”
            if args.help:
                # Creamos un nuevo parser simplificado solo para la ayuda breve
                argparse.ArgumentParser(
                    prog="scanner_v2.py",
                    description="Escáner web rápido + crawler"
                ).print_help(sys.stderr)
            else:
                banner()
            sys.exit(0)

        # Muestra banner salvo que se pida --quiet
        if not args.quiet:
            banner()

        # Validación básica de argumentos posicionales
        if args.target_file is None:
            print("[!] Debes indicar al menos el archivo de objetivos.")
            print("    Usa -h para ver ayuda.")
            sys.exit(1)

        # -- Parseo de filtros --
        hide_words = [int(w) for w in args.hw.split(",") if w.isdigit()]
        hide_codes = [int(c) for c in args.hc.split(",") if c.isdigit()]
        hide_lines = [int(l) for l in args.hl.split(",") if l.isdigit()]

        # -- Carga de archivos --
        targets            = load_targets(args.target_file)
        DEFAULT_XSS = ("/usr/share/wordlists/SecLists/Fuzzing/XSS/"
                    "robot-friendly/XSS-EnDe-xssAttacks.txt")

        if args.xss_wordlist:                 # Se pidió un archivo
            xss_payloads = load_wordlist(args.xss_wordlist)
            if not xss_payloads:              # Está vacío o no existe
                print(f"[!] No se pudo cargar {args.xss_wordlist}. "
                    f"Usando wordlist por defecto: {DEFAULT_XSS}")
                xss_payloads = load_wordlist(DEFAULT_XSS)
        else:                                 # No se pasó argumento → default
            print(f"[i] No se especificó wordlist XSS. Usando por defecto "
                f"{DEFAULT_XSS}")
            xss_payloads = load_wordlist(DEFAULT_XSS)
    # ---------------------------------------------------------------
    # ❹ Carga opcional de directorios
        DEFAULT_DIRS = ("/usr/share/wordlists/SecLists/Discovery/Web-Content/"
                        "directory-list-2.3-medium.txt")
        DEFAULT_PAGES = ("/usr/share/wordlists/SecLists/Fuzzing/"
                        "fuzz-Bo0oM.txt")

        # --- directorios ---
        dir_list_path = args.dir_wordlist or DEFAULT_DIRS
        directory_wordlist = load_wordlist(dir_list_path)
        if not directory_wordlist:
            print(f"[!] Wordlist de directorios vacía: {dir_list_path}")

        # --- páginas/archivos ---
        page_list_path = args.page_wordlist or DEFAULT_PAGES
        page_wordlist = load_wordlist(page_list_path)
        if not page_wordlist:
            print(f"[!] Wordlist de páginas vacía: {page_list_path}")
        # -- Config interactiva (podrías convertirlas en flags) --
        max_depth = int(input("Profundidad máxima de crawling (ej 2): "))
        exclude_patterns = input(
            "Patrones a excluir (comma-sep, ej 'images,css'): "
        ).split(",")

        # -- Bucle de objetivos --
        for target_url in targets:
            print(f"\n[+] Iniciando escaneo para: {target_url}")
            links_to_ignore = [urljoin(target_url, "logout.php")]
            links_to_ignore += [urljoin(target_url, pat) for pat in exclude_patterns]

            scanner = Scanner(threads,
                target_url, links_to_ignore, xss_payloads,
                hide_words, hide_codes, hide_lines, max_depth, exclude_patterns=exclude_patterns
            )
            scanner.crawl(directory_wordlist=directory_wordlist)
            scanner.discover_pages(page_wordlist)
            scanner.run_scanner()
    except KeyboardInterrupt:
        print("\n[✗] Interrumpido por el usuario. Saliendo…")
        finally:
            scanner.session.close()
        sys.exit(0)